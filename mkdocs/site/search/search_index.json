{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes on Google Cloud What's this Welcome to Kubernetes on Google Cloud, the goal of this repo is to offer a collection of best practices and tutorrials for better use of GKE & Anthos products. Currently the content has been categorized for the following subjects: General Networking Storage Day 2 Operation CI/CD Tooling Will include advanced topics in the future: Multi/Hybrid Clusters Solutions on GKE Workaround for something Contributing Encourage you to contribute to this repo if you have implemented a practice that has proven to solve something, just open an issue or a pull request to share with us, please.","title":"Home"},{"location":"#kubernetes-on-google-cloud","text":"","title":"Kubernetes on Google Cloud"},{"location":"#whats-this","text":"Welcome to Kubernetes on Google Cloud, the goal of this repo is to offer a collection of best practices and tutorrials for better use of GKE & Anthos products. Currently the content has been categorized for the following subjects: General Networking Storage Day 2 Operation CI/CD Tooling Will include advanced topics in the future: Multi/Hybrid Clusters Solutions on GKE Workaround for something","title":"What's this"},{"location":"#contributing","text":"Encourage you to contribute to this repo if you have implemented a practice that has proven to solve something, just open an issue or a pull request to share with us, please.","title":"Contributing"},{"location":"about/","text":"","title":"About"},{"location":"old/","text":"GKE 1. Infrastructure 1.1. Place Pods into nodes in single zone with high availability The cluster has multiple node pools for cross different zones, one zone for primary and one for standby. Using Affinity/Anti-affinity to place Pods into nodes in primary zone and shift to standby zone when there's zonal failure. 1.2 Run boortrap scripts when launching nodes in GKE To run bootstrap scripts for your nodes in GKE such as initialize something, add iptable entry, etc., you can run a quick DeamonSet to achieve that. 1.3 Using Cloud DNS instead of Kube DNS Use much more reliable and robust option Cloud DNS (100% SLO) ot replace Kube DNS. Refernce from blog . 1.4 Validating GKE clusters against configuration best practices GKE Policy Automation from Google, contains the tool and the policy library for validating GKE clusters against configuration best practices. 2. Autoscaling 2.1 Scale the cluster with customized Autoscaler 3. Observability 3.1. Managed Promestheus with Dataproc on GKE 4. Networking 4.1 Repalce Ingress by Gateway API 4.2 Anthos Service Mesh (ASM) + Locality Setting Visualize pods across the nodes in different zones to supports highly available and scalable, as well as leverage weight distrubution of Istio to reduce inter-zone traffic & cost. 4.3 GLB + Anthos Service Mesh (ASM) + Locality Setting Using GLB and ASM to implement multi-cluster traffic managment across different Cloud Providers. In each individual k8s cluster we leverage weight distrubution of Istio to reduce inter-zone traffic & cost. 4.4 Multi-cluster mesh outside Google Cloud https://cloud.google.com/service-mesh/docs/unified-install/off-gcp-multi-cluster-setup 5.Storage 5.1 Using Google Cloud Storage https://github.com/GoogleCloudPlatform/gcsfuse 5.2 Using Google Filestore https://github.com/kubernetes-sigs/gcp-filestore-csi-driver 5.3 Using Google Persisten Disk https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver 6. Security 6.1 Setup workload identity on GKE 6.2 Setup acccessing AWS service from GKE through workload identity K3s 1. Infrastructure 1.1 Provision K3s on GCE with CCM K3s is a lightweight Kubernetes distributuion and packaged as a signle binary, is good for IoT, Edge compute, DevOps, customized k8s compute, etc. Just jump into here to know about better leveraging K3s on GCP.","title":"Old"},{"location":"old/#gke","text":"","title":"GKE"},{"location":"old/#1-infrastructure","text":"","title":"1. Infrastructure"},{"location":"old/#11-place-pods-into-nodes-in-single-zone-with-high-availability","text":"The cluster has multiple node pools for cross different zones, one zone for primary and one for standby. Using Affinity/Anti-affinity to place Pods into nodes in primary zone and shift to standby zone when there's zonal failure.","title":"1.1. Place Pods into nodes in single zone with high availability"},{"location":"old/#12-run-boortrap-scripts-when-launching-nodes-in-gke","text":"To run bootstrap scripts for your nodes in GKE such as initialize something, add iptable entry, etc., you can run a quick DeamonSet to achieve that.","title":"1.2 Run boortrap scripts when launching nodes in GKE"},{"location":"old/#13-using-cloud-dns-instead-of-kube-dns","text":"Use much more reliable and robust option Cloud DNS (100% SLO) ot replace Kube DNS. Refernce from blog .","title":"1.3 Using Cloud DNS instead of Kube DNS"},{"location":"old/#14-validating-gke-clusters-against-configuration-best-practices","text":"GKE Policy Automation from Google, contains the tool and the policy library for validating GKE clusters against configuration best practices.","title":"1.4 Validating GKE clusters against configuration best practices"},{"location":"old/#2-autoscaling","text":"","title":"2. Autoscaling"},{"location":"old/#21-scale-the-cluster-with-customized-autoscaler","text":"","title":"2.1 Scale the cluster with customized Autoscaler"},{"location":"old/#3-observability","text":"","title":"3. Observability"},{"location":"old/#31-managed-promestheus-with-dataproc-on-gke","text":"","title":"3.1. Managed Promestheus with Dataproc on GKE"},{"location":"old/#4-networking","text":"","title":"4. Networking"},{"location":"old/#41-repalce-ingress-by-gateway-api","text":"","title":"4.1 Repalce Ingress by Gateway API"},{"location":"old/#42-anthos-service-mesh-asm-locality-setting","text":"Visualize pods across the nodes in different zones to supports highly available and scalable, as well as leverage weight distrubution of Istio to reduce inter-zone traffic & cost.","title":"4.2 Anthos Service Mesh (ASM) + Locality Setting"},{"location":"old/#43-glb-anthos-service-mesh-asm-locality-setting","text":"Using GLB and ASM to implement multi-cluster traffic managment across different Cloud Providers. In each individual k8s cluster we leverage weight distrubution of Istio to reduce inter-zone traffic & cost.","title":"4.3 GLB + Anthos Service Mesh (ASM) + Locality Setting"},{"location":"old/#44-multi-cluster-mesh-outside-google-cloud","text":"https://cloud.google.com/service-mesh/docs/unified-install/off-gcp-multi-cluster-setup","title":"4.4 Multi-cluster mesh outside Google Cloud"},{"location":"old/#5storage","text":"","title":"5.Storage"},{"location":"old/#51-using-google-cloud-storage","text":"https://github.com/GoogleCloudPlatform/gcsfuse","title":"5.1 Using Google Cloud Storage"},{"location":"old/#52-using-google-filestore","text":"https://github.com/kubernetes-sigs/gcp-filestore-csi-driver","title":"5.2 Using Google Filestore"},{"location":"old/#53-using-google-persisten-disk","text":"https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver","title":"5.3 Using Google Persisten Disk"},{"location":"old/#6-security","text":"","title":"6. Security"},{"location":"old/#61-setup-workload-identity-on-gke","text":"","title":"6.1 Setup workload identity on GKE"},{"location":"old/#62-setup-acccessing-aws-service-from-gke-through-workload-identity","text":"","title":"6.2 Setup acccessing AWS service from GKE through workload identity"},{"location":"old/#k3s","text":"","title":"K3s"},{"location":"old/#1-infrastructure_1","text":"","title":"1. Infrastructure"},{"location":"old/#11-provision-k3s-on-gce-with-ccm","text":"K3s is a lightweight Kubernetes distributuion and packaged as a signle binary, is good for IoT, Edge compute, DevOps, customized k8s compute, etc. Just jump into here to know about better leveraging K3s on GCP.","title":"1.1 Provision K3s on GCE with CCM"},{"location":"general/prepare-gke/","text":"Preparing GKE Cluster for Production Code repo GitOps & CI/CD Monitoring Testing SRE References Preparing a Google Kubernetes Engine environment for production","title":"Preparing GKE cluster for PRODUCTION"},{"location":"general/prepare-gke/#preparing-gke-cluster-for-production","text":"Code repo GitOps & CI/CD Monitoring Testing SRE","title":"Preparing GKE Cluster for Production"},{"location":"general/prepare-gke/#references","text":"Preparing a Google Kubernetes Engine environment for production","title":"References"},{"location":"networking/asm-locality/","text":"Description Visualize pods across the nodes in different zones to supports highly available and scalable, as well as leverage weight distrubution of Istio to reduce inter-zone traffic & cost. Deplyment # Clone repo git clone https://github.com/cc4i/multi-k8s.git cd multi-k8s cd hack && ./gke.sh # Provision & deployment skaffold build tag=`skaffold build --dry-run --output='{{json .}}' --quiet |jq '.builds[].tag' -r` skaffold deploy --images ${tag} # Apply locality setting kubectl apply -f manifests/istio -n run-tracker # Run endpoint=`kubectl get svc/istio-ingressgateway -n run-tracker -o \"jsonpath={.status.loadBalancer.ingress[0].ip}\"` curl -v http://${endpoint}/trip |jq Usage The UI to understand accessing map. endpoint=`kubectl get svc/istio-ingressgateway -n run-tracker -o \"jsonpath={.status.loadBalancer.ingress[0].ip}\"` # Access by http://${endpoint}/tracker-ui open http://${endpoint}/tracker-ui Notes","title":"Reduce inter-zone traffic with Anthhos Service Mesh"},{"location":"networking/asm-locality/#_1","text":"","title":""},{"location":"networking/asm-locality/#description","text":"Visualize pods across the nodes in different zones to supports highly available and scalable, as well as leverage weight distrubution of Istio to reduce inter-zone traffic & cost.","title":"Description"},{"location":"networking/asm-locality/#deplyment","text":"# Clone repo git clone https://github.com/cc4i/multi-k8s.git cd multi-k8s cd hack && ./gke.sh # Provision & deployment skaffold build tag=`skaffold build --dry-run --output='{{json .}}' --quiet |jq '.builds[].tag' -r` skaffold deploy --images ${tag} # Apply locality setting kubectl apply -f manifests/istio -n run-tracker # Run endpoint=`kubectl get svc/istio-ingressgateway -n run-tracker -o \"jsonpath={.status.loadBalancer.ingress[0].ip}\"` curl -v http://${endpoint}/trip |jq","title":"Deplyment"},{"location":"networking/asm-locality/#usage","text":"The UI to understand accessing map. endpoint=`kubectl get svc/istio-ingressgateway -n run-tracker -o \"jsonpath={.status.loadBalancer.ingress[0].ip}\"` # Access by http://${endpoint}/tracker-ui open http://${endpoint}/tracker-ui","title":"Usage"},{"location":"networking/asm-locality/#notes","text":"","title":"Notes"},{"location":"networking/glb-locality/","text":"Description Using GLB and ASM to implement multi-cluster traffic managment across different Cloud Providers. In each individual k8s cluster we leverage weight distrubution of Istio to reduce inter-zone traffic & cost. Deployment Networking Notes Reference document : Global load balance with hybrid connectivity","title":"Hybrid connectivity with services on AWS EKS"},{"location":"networking/glb-locality/#_1","text":"","title":""},{"location":"networking/glb-locality/#description","text":"Using GLB and ASM to implement multi-cluster traffic managment across different Cloud Providers. In each individual k8s cluster we leverage weight distrubution of Istio to reduce inter-zone traffic & cost.","title":"Description"},{"location":"networking/glb-locality/#deployment","text":"","title":"Deployment"},{"location":"networking/glb-locality/#networking","text":"","title":"Networking"},{"location":"networking/glb-locality/#notes","text":"Reference document : Global load balance with hybrid connectivity","title":"Notes"},{"location":"networking/single-zone/","text":"Description Placing pods into nodes in single zone with high availability, there're two node pools, one is the primary node pool in signle zone, other one is standby node pool with minmum number of instance is zero. Deployment # Clone repo git clone https://github.com/cc4i/multi-k8s.git cd multi-k8s cd asset/tod/bin && ./gke-affinity.sh # Apply manifests cd .. kubectl apply -f manifests/ns.yaml kubectl apply -f manifests/service-account.yaml kubectl apply -f deploy-affinity kubectl apply -f service Notes Affinity is a feature from upstream Kubernetes.","title":"Deploy workload into single zone with high availability"},{"location":"networking/single-zone/#_1","text":"","title":""},{"location":"networking/single-zone/#description","text":"Placing pods into nodes in single zone with high availability, there're two node pools, one is the primary node pool in signle zone, other one is standby node pool with minmum number of instance is zero.","title":"Description"},{"location":"networking/single-zone/#deployment","text":"# Clone repo git clone https://github.com/cc4i/multi-k8s.git cd multi-k8s cd asset/tod/bin && ./gke-affinity.sh # Apply manifests cd .. kubectl apply -f manifests/ns.yaml kubectl apply -f manifests/service-account.yaml kubectl apply -f deploy-affinity kubectl apply -f service","title":"Deployment"},{"location":"networking/single-zone/#notes","text":"Affinity is a feature from upstream Kubernetes.","title":"Notes"},{"location":"security/workload-idetity/","text":"Security 1 Setup workload identity on GKE 2 Setup acccessing AWS service from GKE through workload identity","title":"Acccessing AWS services from GKE through workload identity"},{"location":"security/workload-idetity/#security","text":"","title":"Security"},{"location":"security/workload-idetity/#1-setup-workload-identity-on-gke","text":"","title":"1 Setup workload identity on GKE"},{"location":"security/workload-idetity/#2-setup-acccessing-aws-service-from-gke-through-workload-identity","text":"","title":"2 Setup acccessing AWS service from GKE through workload identity"},{"location":"solutions/play-k3s/","text":"K3s on GCP Provision K3s on GCE with Cloud Controller Manager (CCM) Key points: - Grant server & agent nodes with permission in order to provision network resources. - Allow port:60000 in the firewall rules. - Following guidance to build CCM and push into your own image registry. - Configure RBAC and allow CCM working properly, see reference . Steps - Create service account with proper permissions. - Create instance template for K3s server. - Create managed instance group with server template. - Install K3s server side. - Create agent instances template for K3s agent. - Cerate managed instance group with agent template. - Taint server node. - Deploy CCM for GCE into K3s cluster. All-in-one quick-start example The k3s cluster uses managed instance group for both server and agent nodes for high available purpose, but no auto scaling capability, no external database. git clone https://github.com/cc4i/multi-k8s.git cd multi-k8s/hack && ./k3s-quickstart.sh High-Availability and auto-scalability K3s Cluster The K3s cluster leverages managed instance group for both server and agent nodes, integrate with internal load balance with server nodes to provide fixed registeration address. Configure auto scaling policy with CPU utilization by default, using Cloud SQL as external database. Clean up cd multi-k8s/hack && ./k3s-quickstart-cleanup.sh","title":"K3s integrate Google Cloud Provider on GCE"},{"location":"solutions/play-k3s/#k3s-on-gcp","text":"","title":"K3s on GCP"},{"location":"solutions/play-k3s/#provision-k3s-on-gce-with-cloud-controller-manager-ccm","text":"Key points: - Grant server & agent nodes with permission in order to provision network resources. - Allow port:60000 in the firewall rules. - Following guidance to build CCM and push into your own image registry. - Configure RBAC and allow CCM working properly, see reference . Steps - Create service account with proper permissions. - Create instance template for K3s server. - Create managed instance group with server template. - Install K3s server side. - Create agent instances template for K3s agent. - Cerate managed instance group with agent template. - Taint server node. - Deploy CCM for GCE into K3s cluster.","title":"Provision K3s on GCE with Cloud Controller Manager (CCM)"},{"location":"solutions/play-k3s/#all-in-one-quick-start-example","text":"The k3s cluster uses managed instance group for both server and agent nodes for high available purpose, but no auto scaling capability, no external database. git clone https://github.com/cc4i/multi-k8s.git cd multi-k8s/hack && ./k3s-quickstart.sh","title":"All-in-one quick-start example"},{"location":"solutions/play-k3s/#high-availability-and-auto-scalability-k3s-cluster","text":"The K3s cluster leverages managed instance group for both server and agent nodes, integrate with internal load balance with server nodes to provide fixed registeration address. Configure auto scaling policy with CPU utilization by default, using Cloud SQL as external database.","title":"High-Availability and auto-scalability K3s Cluster"},{"location":"solutions/play-k3s/#clean-up","text":"cd multi-k8s/hack && ./k3s-quickstart-cleanup.sh","title":"Clean up"},{"location":"workaround/startup-script/","text":"Run boortrap scripts when launching nodes in GKE Instruction # Clone repo git clone https://github.com/cc4i/multi-k8s.git cd bootstrap/startup-script # build image & push into registry docker build . -t <image url>:<tag> docker push <image url>:<tag> # Modify script as per request betweend \"# BO:\" and \"# EO:\" # Deploy to cluster kustomize build . | kubectl apply -f - Notes The pod container can share the host process ID namespace. THe container is required root privilege.","title":"Run a boortrap script when launching nodes in GKE"},{"location":"workaround/startup-script/#run-boortrap-scripts-when-launching-nodes-in-gke","text":"","title":"Run boortrap scripts when launching nodes in GKE"},{"location":"workaround/startup-script/#instruction","text":"# Clone repo git clone https://github.com/cc4i/multi-k8s.git cd bootstrap/startup-script # build image & push into registry docker build . -t <image url>:<tag> docker push <image url>:<tag> # Modify script as per request betweend \"# BO:\" and \"# EO:\" # Deploy to cluster kustomize build . | kubectl apply -f -","title":"Instruction"},{"location":"workaround/startup-script/#notes","text":"The pod container can share the host process ID namespace. THe container is required root privilege.","title":"Notes"}]}