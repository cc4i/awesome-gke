{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#kubernetes-on-google-cloud","title":"Kubernetes on Google Cloud","text":""},{"location":"#whats-this","title":"What's this","text":"<p>Welcome to Kubernetes on Google Cloud, the goal of this repo is to offer a collection of best practices and tutorrials for better use of GKE &amp; Anthos products. Currently the content has been categorized for the following subjects:</p> <ul> <li>General</li> <li>Networking </li> <li>Storage</li> <li>Day 2 Operation </li> <li>CI/CD</li> <li>Tooling</li> </ul> <p>Will include advanced topics in the future: </p> <ul> <li>Multi/Hybrid Clusters</li> <li>Solutions on GKE</li> <li>Workaround for something </li> </ul> <p>Live docs</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Encourage you to contribute to this repo if you have implemented a practice that has proven to solve something, just open an issue or a pull request to share with us, please.</p>"},{"location":"#notes","title":"Notes","text":"<p>Starring the repo if you feel helpful, that's the big motivation and thanks.</p>"},{"location":"license/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol>"},{"location":"general/prepare-gke/","title":"1. Preparing GKE cluster for PRODUCTION","text":""},{"location":"general/prepare-gke/#checklist-for-gke","title":"Checklist for GKE","text":"<ul> <li> <p>[X] Have a high level architecture of your applications and map them into a Kubernetes cluster, think about how they're going to deploy and run. </p> </li> <li> <p>[X] Public vs priavte GKE Cluster</p> <ul> <li>Control plane (API) endpoint accessbility</li> <li>Public vs. private IPs for nodes</li> <li>Node service accounts</li> <li>Accessibility of node metadata</li> </ul> </li> <li> <p>[X] Planning networking</p> <ul> <li>Plan Pod density per node</li> <li>Planning IP ranges for: nodes, pods, services, control plane, load balancer (maybe), etc.</li> <li>Routes-based or VPC-native</li> <li>Single VPC, shared VPC, VPC Peering, or Open-Hybrid</li> <li>NATs</li> </ul> </li> <li> <p>[X] Cluster security</p> <ul> <li>Restrict access to the control plane</li> <li>Limit access with IAM service accounts</li> <li>Periodically rotate credentials</li> <li>Secret encryption</li> <li>Isolation based on communication needs for multi-tenancy</li> <li>Workload identity</li> <li>RBAC</li> </ul> </li> <li> <p>[X] Capacity and scalability</p> <ul> <li>Capacity of nodes based on your requirement, such as CPU, GPU or TPU, etc.</li> <li>Node Auto Provison (NAP)</li> <li>Node pool autoscaling</li> <li>Horizontal Pod autoscaling (HPA)</li> <li>Vertical Pod autoscaling (VPA)</li> <li>Multidimensional Pod autoscaling</li> <li>Customized cluster autoscaler with community edition</li> </ul> </li> <li> <p>[X] Storage</p> <ul> <li>Zonal or regional persistent storage</li> <li>Shared storage</li> <li>Object storage</li> <li>Require curtain IOPS of storage </li> </ul> </li> <li> <p>[X] Operational capability</p> <ul> <li>Enroll to a right channel: Rapid, Regular or Stable</li> <li>GKE or GKE Autopilot</li> <li>Zonal or regional cluster</li> <li>Maintainace window</li> <li>Metrics, logging and tracing</li> <li>Backup for GKE</li> <li>Runtime security monitoring - Security Posture</li> <li>CI/CD</li> </ul> </li> <li> <p>[X] Running services on GKE</p> <ul> <li>Using Service Mesh</li> <li>Multi-cluster</li> <li>Service discovery across multiple clusters</li> <li>KubeDNS or Cloud DNS</li> <li>Configuration</li> <li>Ingress or Gateway API</li> </ul> </li> </ul>"},{"location":"general/prepare-gke/#references","title":"References","text":"<ul> <li>Smooth sailing with Kubernetes</li> <li>Preparing a Google Kubernetes Engine environment for production </li> </ul>"},{"location":"networking/asm-locality/","title":"2. Reduce interzone traffic with Anthhos Service Mesh","text":""},{"location":"networking/asm-locality/#description","title":"Description","text":"<p>Visualize pods across the nodes in different zones to supports highly available and scalable, as well as leverage weight distrubution of Istio to reduce inter-zone traffic &amp; cost.</p>"},{"location":"networking/asm-locality/#deplyment","title":"Deplyment","text":"<pre><code># Clone repo\ngit clone https://github.com/cc4i/awesome-gke.git\ncd awesome-gke\ncd asset/tod/bin &amp;&amp; ./gke.sh\n# Execute printed commands to install Anthos Services Mesh\n\n# Provision &amp; deployment\nskaffold build \ntag=`skaffold build --dry-run --output='{{json .}}' --quiet |jq '.builds[].tag' -r`\nskaffold deploy --images ${tag}\n\n# Run\nendpoint=`kubectl get svc/istio-ingressgateway -n run-tracker -o \"jsonpath={.status.loadBalancer.ingress[0].ip}\"`\ncurl -v http://${endpoint}/trip |jq\n\n</code></pre>"},{"location":"networking/asm-locality/#usage","title":"Usage","text":"<p>The UI to understand accessing map. </p> <pre><code>endpoint=`kubectl get svc/istio-ingressgateway -n run-tracker -o \"jsonpath={.status.loadBalancer.ingress[0].ip}\"`\n\n# Access by http://${endpoint}/tracker-ui\nopen http://${endpoint}/tracker-ui\n\n</code></pre>"},{"location":"networking/asm-locality/#notes","title":"Notes","text":"<ul> <li>Install script was built to run on Linux, so run them in Cloud Shell or Linux to install Anthos Services Mesh.</li> </ul>"},{"location":"networking/clouddns/","title":"6. Enhance domain resolving with Cloud DNS","text":""},{"location":"networking/clouddns/#description","title":"Description","text":"<p>To improve domain resolving performance and experience in GKE, we can integrate Cloud DNS with GKE instead of using KubeDNS.</p>"},{"location":"networking/clouddns/#guide","title":"Guide","text":"<pre><code>\n# 1. Setup environment variables, replace with your own\n\nexport PROJECT_ID=&lt;Project ID&gt;\nexport ZONE=&lt;Zone&gt;\nexport REGION=&lt;Region&gt;\nexport CLUSTER=clouddns-cluster\n\n# 2. Provision GKE with cluster scope DNS\n\ngcloud beta container clusters create clouddns-cluster \\\n    --cluster-dns=clouddns \\\n    --cluster-dns-scope=cluster \\\n    --zone=${ZONE}\n\n\n# 3. Verify Cloud DNS\n\nkubectl run dns-test --image us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\nkubectl expose pod dns-test --name dns-test-svc --port 8080\nkubectl get svc dns-test-svc\nkubectl exec -it dns-test -- nslookup dns-test-svc\n\n# 4. Add A record inside of Cloud DNS, such as \"server-kafka-0.server-kafka-headless.default.svc : 192.0.2.99\", then you can use follwing command to validate \n\nkubectl exec -it dns-test -- nslookup server-kafka-0.server-kafka-headless.default.svc.cluster.local\n\n</code></pre>"},{"location":"networking/clouddns/#references","title":"References","text":"<ul> <li>https://medium.com/google-cloud/dns-on-gke-everything-you-need-to-know-b961303f9153</li> <li>https://cloud.google.com/kubernetes-engine/docs/how-to/cloud-dns</li> </ul>"},{"location":"networking/gateway-hands-on/","title":"Gateway API Hands-on Labs","text":""},{"location":"networking/gateway-hands-on/#overview","title":"Overview","text":"<p>The sample labs in this section demonstrate how to use the Gateway API to configure a Gateway and expose a service to external traffic. The labs are based on the Gateway API and GKE Gateway features.</p>"},{"location":"networking/gateway-hands-on/#contents","title":"Contents","text":"<ul> <li>Lab1. External Gateway using Certificate Manager with wildcard domain</li> <li>Lab2. Configuring a static IP for a Gateway</li> <li>Lab3. Multi-cluster gateway</li> <li>Lab4. Capacity-based load balancing</li> </ul>"},{"location":"networking/gateway-hands-on/#prerequisites","title":"Prerequisites","text":""},{"location":"networking/gateway-hands-on/#1-enable-apis-such-as-gke-gke-hub-and-gke-multi-cluster-ingress-mci-etc-in-your-projects","title":"1. Enable APIs, such as GKE, GKE Hub, and GKE Multi-cluster Ingress (MCI), etc. in your projects","text":"<pre><code># Environment variables\nexport PROJECT_ID=play-api-service\nexport PROJECT_NUMBER=374299782509\n\n# Enable APIs\ngcloud services enable \\\n    container.googleapis.com \\\n    gkehub.googleapis.com \\\n    multiclusterservicediscovery.googleapis.com \\\n    multiclusteringress.googleapis.com \\\n    trafficdirector.googleapis.com \\\n    --project=${PROJECT_ID}\n</code></pre>"},{"location":"networking/gateway-hands-on/#2-provision-a-gke-cluster-with-gateway-api-or-enable-existed-gke-clutser","title":"2. Provision a GKE cluster with Gateway API, or enable existed GKE clutser","text":"<pre><code># Create a GKE cluster with Gateway API\ngcloud container clusters create pt-cluster-4 \\\n    --gateway-api=standard \\\n    --release-channel=Rapid \\\n    --region=asia-southeast1\n\n\n# Enable Gateway API for an existing GKE cluster\ngcloud container clusters update pt-cluster-4 \\\n    --gateway-api=standard \\\n    --region=asia-southeast1\n\n# Verify the cluster\nkubectl get gatewayclass\n</code></pre>"},{"location":"networking/gateway-hands-on/#3-checkout-the-sample-code","title":"3. Checkout the sample code","text":"<pre><code>git clone https://github.com/cc4i/awesome-gke.git\ncd awesome-gke/gatewayclass\n</code></pre>"},{"location":"networking/gateway-hands-on/#lab-1-external-gateway-using-certificate-manager-with-wildcard-domain","title":"Lab 1. External Gateway using Certificate Manager with wildcard domain","text":""},{"location":"networking/gateway-hands-on/#11-create-a-certificate-map","title":"1.1 Create a certificate map","text":"<pre><code># Enable Certificate Manager API\ngcloud services enable certificatemanager.googleapis.com\n\n# Create a DNS Authorization to validate your certificate. Example here is my personal domain cc4i.xyz and reigstered in GoDaddy, you can use your own domain.\ngcloud certificate-manager dns-authorizations create dns-auth-cc4i-xyz \\\n  --domain=\"cc4i.xyz\"\n\n# !!!Returning DNS resource record needs to be added as CNAME into your DNS configuration !!!\n\n# Create a certificate\ngcloud beta certificate-manager certificates create store-cc4i-xyz-cert \\\n    --domains=\"cc4i.xyz,*.cc4i.xyz\" \\\n    --dns-authorizations=dns-auth-cc4i-xyz\n\n# Create a certificate map\ngcloud beta certificate-manager maps create store-cc4i-xyz-map\n\n# Create a certificate map entry for wildcard domain\ngcloud beta certificate-manager maps entries create store-cc4i-xyz-map-entry1 \\\n    --map=store-cc4i-xyz-map \\\n    --hostname=\"*.cc4i.xyz\" \\\n    --certificates=store-cc4i-xyz-cert\n\n# Create a certificate map entry for root domain\ngcloud beta certificate-manager maps entries create store-cc4i-xyz-map-entry2 \\\n    --map=store-cc4i-xyz-map \\\n    --hostname=\"cc4i.xyz\" \\\n    --certificates=store-cc4i-xyz-cert\n</code></pre> <p>References:  - DNS Authorization - Secure a gateway</p>"},{"location":"networking/gateway-hands-on/#12-create-a-gateway","title":"1.2 Create a Gateway","text":"<pre><code># Create a Gateway\nkubectl apply -f single-https/gateway.yaml\n\n# Verify the Gateway\nkubectl describe gateways.gateway.networking.k8s.io external-http\n</code></pre>"},{"location":"networking/gateway-hands-on/#13-deployment-demo-application","title":"1.3 Deployment demo application","text":"<pre><code># Deploy the demo application\nkubectl apply -f single-https/store.yaml\nkubectl get pods\nkubectl get service\n\n# Check Gateway IP\nkubectl get gateways.gateway.networking.k8s.io external-http -o=jsonpath=\"{.status.addresses[0].value}\"\n\n# Apply the http route\nkubectl apply -f single-https/store-route-external.yaml\n</code></pre> <p>References: - Demo applications - HTTPRoute</p>"},{"location":"networking/gateway-hands-on/#14-use-shared-gateways","title":"1.4 Use shared Gateways","text":"<pre><code># Used as a shared Gateway\nkubectl apply -f single-https/site.yaml\n# Apply the http route\nkubectl apply -f single-https/site-route-external.yaml\n</code></pre>"},{"location":"networking/gateway-hands-on/#lab-2-configuring-a-static-ip-for-a-gateway","title":"Lab 2. Configuring a static IP for a Gateway","text":""},{"location":"networking/gateway-hands-on/#21-create-a-static-ip-address","title":"2.1 Create a static IP address","text":"<pre><code># https://cloud.google.com/kubernetes-engine/docs/how-to/deploying-gateways#gateway_ip_addressing\n\n\ngcloud compute addresses create test-public-ip-cc4i-xyz \\\n    --global \\\n    --project=${PROJECT_ID}\n</code></pre>"},{"location":"networking/gateway-hands-on/#22-create-a-gateway-with-static-ip","title":"2.2 Create a Gateway with static IP","text":"<pre><code># Apply the Gateway with static IP\nkubectl apply -f single-https/named-ip-gateway.yaml \n</code></pre>"},{"location":"networking/gateway-hands-on/#lab-3-multi-cluster-gateway","title":"Lab 3. Multi-cluster gateway","text":""},{"location":"networking/gateway-hands-on/#31-craete-multiple-gke-clusters-in-different-regions","title":"3.1 Craete multiple GKE clusters in different regions","text":"<pre><code># Create multiple GKE clusters in different regions\ngcloud container clusters create gke-west-1 \\\n    --gateway-api=standard \\\n    --zone=us-west1-a \\\n    --workload-pool=${PROJECT_ID}.svc.id.goog \\\n    --cluster-version=1.25.6-gke.1000 \\\n    --project=${PROJECT_ID}\n\ngcloud container clusters create gke-west-2 \\\n    --gateway-api=standard \\\n    --zone=us-west1-a \\\n    --workload-pool=${PROJECT_ID}.svc.id.goog \\\n    --cluster-version=1.25.6-gke.1000 \\\n    --project=${PROJECT_ID}\n\ngcloud container clusters create gke-east-1 \\\n    --gateway-api=standard \\\n    --zone=us-east1-b \\\n    --workload-pool=${PROJECT_ID}.svc.id.goog \\\n    --cluster-version=1.25.6-gke.1000 \\\n    --project=${PROJECT_ID}\n\n# Rename the context\nkubectl config rename-context gke_${PROJECT_ID}_us-west1-a_gke-west-1 gke-west-1\nkubectl config rename-context gke_${PROJECT_ID}_us-west1-a_gke-west-2 gke-west-2\nkubectl config rename-context gke_${PROJECT_ID}_us-east1-b_gke-east-1 gke-east-1\n</code></pre>"},{"location":"networking/gateway-hands-on/#32-register-to-the-fleet","title":"3.2 Register to the fleet","text":"<pre><code># Register to the fleet\ngcloud container fleet memberships register gke-west-1 \\\n     --gke-cluster us-west1-a/gke-west-1 \\\n     --enable-workload-identity \\\n     --project=${PROJECT_ID}\n\ngcloud container fleet memberships register gke-west-2 \\\n     --gke-cluster us-west1-a/gke-west-2 \\\n     --enable-workload-identity \\\n     --project=${PROJECT_ID}\n\ngcloud container fleet memberships register gke-east-1 \\\n     --gke-cluster us-east1-b/gke-east-1 \\\n     --enable-workload-identity \\\n     --project=${PROJECT_ID}\n\n# Try following command to add permission to service account if you got error ralated to permission!!!\ngcloud projects add-iam-policy-binding ${PROJECT_ID} --member \\\n    serviceAccount:service-${PROJECT_NUMBER}@gcp-sa-gkehub.iam.gserviceaccount.com \\\n    --role \"roles/gkehub.connect\"\n\n# Validate the membership\ngcloud container fleet memberships list --project=${PROJECT_ID}\n\n</code></pre>"},{"location":"networking/gateway-hands-on/#33-enable-multi-cluster-services","title":"3.3 Enable multi-cluster services","text":"<pre><code># Enable multi-cluster services API\ngcloud container fleet multi-cluster-services enable \\\n    --project ${PROJECT_ID}\n\n# Grant IAM permission required for multi-cluster services\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n     --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[gke-mcs/gke-mcs-importer]\" \\\n     --role \"roles/compute.networkViewer\" \\\n     --project=${PROJECT_ID}\n\n# Choose member cluster, which would host the resource for multi-cluster Gateway\ngcloud container fleet ingress enable \\\n    --config-membership=gke-west-1 \\\n    --project=${PROJECT_ID}\n\n# Validate the ingress\ngcloud container fleet ingress describe --project=${PROJECT_ID}\n\n# Grant IAM permission required for multi-cluster services\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member \"serviceAccount:service-${PROJECT_NUMBER}@gcp-sa-multiclusteringress.iam.gserviceaccount.com\" \\\n    --role \"roles/container.admin\" \\\n    --project=${PROJECT_ID}\n\n# Validate the GatewayClass\nkubectl get gatewayclasses --context=gke-west-1\n</code></pre>"},{"location":"networking/gateway-hands-on/#34-deploy-demo-application-into-all-clusters","title":"3.4 Deploy demo application into all clusters","text":"<pre><code># Deploy demo application into all clusters\nkubectl apply --context gke-west-1 -f multi/multi-store.yaml\nkubectl apply --context gke-west-2 -f multi/multi-store.yaml\nkubectl apply --context gke-east-1 -f multi/multi-store.yaml\n\nkubectl apply -f store-west-service.yaml --context gke-west-1\nkubectl apply -f store-east-service.yaml --context gke-east-1 --namespace store\n\n# Validate the deployment\nkubectl get serviceexports --context gke-east-1 -n store\nkubectl get serviceimports --context gke-east-1 -n store\n</code></pre>"},{"location":"networking/gateway-hands-on/#35-deploy-external-http-gateway","title":"3.5 Deploy external http Gateway","text":"<pre><code># Deploy external http Gateway\nkubectl apply -f multi/external-http-gateway.yaml --context gke-west-1 --namespace store\n\n# Apply the http route\nkubectl apply -f multi/public-route-external.yaml --context gke-west-1 --namespace store\n\n# Validate the Gateway\nkubectl describe gateways.gateway.networking.k8s.io external-http --context gke-west-1 --namespace store\n\n</code></pre>"},{"location":"networking/gateway-hands-on/#36-validate-deployment","title":"3.6 Validate deployment","text":"<pre><code># Get the IP of the Gateway\nkubectl get gateway -n store\nkubectl get gateways.gateway.networking.k8s.io external-http \\\n    -o=jsonpath=\"{.status.addresses[0].value}\" --context gke-west-1 --namespace store\n\n# Take a while to sync all routes into GLB\n\ncurl -H \"host: store.example.com\" http://34.98.80.105\ncurl -H \"host: store.example.com\" http://34.98.80.105/west\ncurl -H \"host: store.example.com\" http://34.98.80.105/east\n</code></pre> <p>Reference: - Enable Multi-cluster Gateway - Fleet</p>"},{"location":"networking/gateway-hands-on/#lab-4-capacity-based-load-balancing","title":"Lab 4. Capacity-based load balancing","text":""},{"location":"networking/gateway-hands-on/#41-prepare-clusters-if-not-done-in-previous-steps","title":"4.1 Prepare clusters if not done in previous steps","text":"<pre><code>kubectl get gatewayclasses --context=gke-west-1\n</code></pre>"},{"location":"networking/gateway-hands-on/#42-deploy-the-demo-application","title":"4.2 Deploy the demo application","text":"<pre><code># Deploy the application into all clusters\nkubectl apply -f capacity-based-glb/store-traffic-deploy.yaml --context gke-west-1\nkubectl apply -f capacity-based-glb/store-traffic-deploy.yaml  --context gke-east-1\nkubectl apply -f capacity-based-glb/store-service.yaml --context gke-west-1\nkubectl apply -f capacity-based-glb store-service.yaml --context gke-east-1\n</code></pre>"},{"location":"networking/gateway-hands-on/#43-deploy-external-http-gateway","title":"4.3 Deploy external http Gateway","text":"<pre><code># Deploy external http Gateway\nkubectl apply -f store-route.yaml --context gke-west-1\n# Validate the Gateway\nkubectl describe gateways.gateway.networking.k8s.io store -n traffic-test --context gke-west-1\n</code></pre>"},{"location":"networking/gateway-hands-on/#44-testing-the-gateway","title":"4.4 Testing the Gateway","text":"<pre><code># Get the IP of the Gateway\ncurl http://34.160.103.232\n\n# Take a while to sync all routes into GLB\nkubectl get gateways.gateway.networking.k8s.io store -n traffic-test \\\n    --context=gke-west-1 \\\n    -o=jsonpath=\"{.metadata.annotations.networking\\.gke\\.io/url-maps}\"\n\n\n# Add a load generator to the cluster\nkubectl run --context=gke-west-1 -i --tty --rm loadgen  \\\n    --image=cyrilbkr/httperf  \\\n    --restart=Never  \\\n    -- /bin/sh -c 'httperf  \\\n    --server=34.160.103.232  \\\n    --hog --uri=\"/zone\" --port 80  --wsess=100000,1,1 --rate 10'\n\n\n# Fetch metrics from Monitoring &amp; observe the traffic distribution\nfetch https_lb_rule\n| metric 'loadbalancing.googleapis.com/https/backend_request_count'\n| filter (resource.url_map_name =='gkemcg1-traffic-test-store-armvfyupay1t')\n| align rate(1m)\n| every 1m\n| group_by [resource.backend_scope],\n    [value_backend_request_count_aggregate:\n       aggregate(value.backend_request_count)]\n\n\n# Add more traffic to cluster &amp; observe the traffic distribution\nkubectl run --context=gke-west-1 -i --tty --rm loadgen  \\\n    --image=cyrilbkr/httperf  \\\n    --restart=Never  \\\n    -- /bin/sh -c 'httperf  \\\n    --server=34.160.103.232  \\\n    --hog --uri=\"/zone\" --port 80  --wsess=100000,1,1 --rate 30'\n</code></pre> <p>Reference: - Capacity-based load balancing</p>"},{"location":"networking/glb-locality/","title":"3. Hybrid connectivity with services on AWS","text":""},{"location":"networking/glb-locality/#description","title":"Description","text":"<p>Using GLB and ASM to implement multi-cluster traffic managment across different Cloud Providers. In each individual k8s cluster we leverage weight distrubution of Istio to reduce inter-zone traffic &amp; cost.</p>"},{"location":"networking/glb-locality/#deployment","title":"Deployment","text":"<pre><code># Clone repo\ngit clone https://github.com/cc4i/awesome-gke.git\ncd awesome-gke\ncd asset/tod/bin &amp;&amp; ./gke.sh\n./eks.sh\n\n\n# !!!Set Kube Context for cluster in GCP &amp; AWS, and install ASM for each of them.!!!\n\n# GCP: \n#   gcloud container clusters get-credentials gke-istio --region asia-southeast1\n#   Rename context -&gt; cluster_name\n#   Install ASM for GKE:  https://cloud.google.com/service-mesh/docs/unified-install/install-anthos-service-mesh#install_mesh_ca\n\n# AWS:\n#   aws eks update-kubeconfig --name eks-sculpture\n#   Rename context -&gt; cluster_name\n#   Install ASM for EKS: https://cloud.google.com/service-mesh/docs/unified-install/install-anthos-service-mesh#install_istio_ca\n\n\n#  Build application \nskaffold build \ntag=`skaffold build --dry-run --output='{{json .}}' --quiet |jq '.builds[].tag' -r`\n\n\n# Switch Kube Conetxt &amp; deploy to GKE\nskaffold deploy --images ${tag}\n\n# Switch Kube Conetxt &amp; deploy to EKS\nskaffold deploy --images ${tag}\n\n</code></pre>"},{"location":"networking/glb-locality/#usage","title":"Usage","text":"<p>Access UI from either cluster and input initial URI, such as \"gcp::192.168.0.2,aws::192.168.0.4\", in order to inital Sankey.</p> <pre><code>\n# Access by http://${endpoint}/tracker-ui\nopen http://${endpoint}/tracker-ui\n\n</code></pre>"},{"location":"networking/glb-locality/#networking","title":"Networking","text":""},{"location":"networking/glb-locality/#notes","title":"Notes","text":"<ul> <li>Install script was built to run on Linux, so run them in Cloud Shell or Linux to install Anthos Services Mesh.</li> <li>Reference document : Global load balance with hybrid connectivity</li> </ul>"},{"location":"networking/glb-static-ip/","title":"7. Binding a static IP","text":""},{"location":"networking/glb-static-ip/#description","title":"Description","text":"<p>How to use Google Kubernetes Engine (GKE) to expose your application to the internet on a static external IP address and configure a domain name to point to your application.</p>"},{"location":"networking/glb-static-ip/#servicel4","title":"Service/L4","text":""},{"location":"networking/glb-static-ip/#ingressl7","title":"Ingress/L7","text":""},{"location":"networking/glb-static-ip/#references","title":"References","text":"<ul> <li>https://cloud.google.com/kubernetes-engine/docs/tutorials/configuring-domain-name-static-ip</li> </ul>"},{"location":"networking/recipes/","title":"4. GKE Network recipes from Google Cloud","text":""},{"location":"networking/recipes/#description","title":"Description","text":"<p>All kinds of network recipes for GKE from offical Google Cloud repositories, excellent examples align with those use cases, which demonstrate how and when those GKE capabilities should be used. </p>"},{"location":"networking/recipes/#references","title":"References","text":"<ul> <li>GKE Networking Recipes</li> </ul>"},{"location":"networking/single-zone/","title":"1. Runing in single zone with high availability","text":""},{"location":"networking/single-zone/#description","title":"Description","text":"<p>Placing pods into nodes in single zone with high availability, there're two node pools, one is the primary node pool in signle zone, other one is standby node pool with minmum number of instance is zero.</p>"},{"location":"networking/single-zone/#deployment","title":"Deployment","text":"<pre><code>\n# Clone repo\ngit clone https://github.com/cc4i/awesome-gke.git\ncd awesome-gke\ncd asset/tod/bin &amp;&amp; ./gke-affinity.sh\n\n# Apply manifests\ncd ../manifests/examples/single-zone\nkustomize build . |kubectl apply -f -\n\n# \nendpoint=`kubectl get svc/svc-1 -n run-tracker -o \"jsonpath={.status.loadBalancer.ingress[0].ip}\"`\n# Access by http://${endpoint}/tracker-ui\nopen http://${endpoint}:8000/tracker-ui\n\n</code></pre>"},{"location":"networking/single-zone/#notes","title":"Notes","text":"<ul> <li>Affinity is a feature from upstream Kubernetes. </li> </ul>"},{"location":"networking/weight-tcp/","title":"5. Weight-based TCP routing","text":""},{"location":"networking/weight-tcp/#description","title":"Description","text":"<p>Control TCP L4 communication is very important in Kubernetes world, such as migrating TCP traffic gradually from an older version of a microservice to a new one. In GKE, you can accomplish that by leveraging Anthos Service Mesh and shfiting a percentage of TCP traffic from one destination to another. </p>"},{"location":"networking/weight-tcp/#guide","title":"Guide","text":"<pre><code># Clone repo\ngit clone https://github.com/cc4i/awesome-gke.git\ncd awesome-gke/asset/tod/bin &amp;&amp; ./gke.sh\n\n\n# Following pinting to install ASM for GKE, https://cloud.google.com/service-mesh/docs/unified-install/install-anthos-service-mesh#install_mesh_ca\n\n\n#  Build application \nskaffold build \ntag=`skaffold build --dry-run --output='{{json .}}' --quiet |jq '.builds[].tag' -r`\n\n\n# Deploy demo applications\ncd awesome-gke/asset/tod/manifests/examples/tcp\nkustomize build . | kubectl apply -f -\n\n# Modify ../../weight-tcp/serving.yaml as by comments\n\n# Validate through 'telnet' \ntelnet &lt;ingress-gateway&gt; 8008\n# Modify weight and observe the status of current connection &amp; new connection.\n\n</code></pre>"},{"location":"networking/weight-tcp/#operate-with-external-tcp-proxy-load-balancer","title":"Operate with External TCP Proxy Load Balancer","text":"<p>We can use External TCP Proxy Load Balancer with NEG to achieve that, but it's much tedious tasks.</p> <p>Create NEG(Network Endpoint Group) with annotations, like following code snippet:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    cloud.google.com/neg: '{\"exposed_ports\": {\"8008\":{\"name\": \"svc-1-v2-tcp-neg\"}}}'\n\n</code></pre> <p>And then create backend service with those NEGs, follow by forwarding rule with TCP protocol. Whenever we need to deploy new version, we can deployment new version with different labels, create new NEGs. Manipulate those NEGs, remove older one and add new one. We can leverage Connection Draining Timeout (maxmium 3600 seconds) to wait flight connection to complete and do not accept new requests during a connection drain.</p> <p>There's potential connection risk to disconnect with old service during update configuration of backend. Change was slow compare to weight-based technique.</p>"},{"location":"networking/weight-tcp/#references","title":"References","text":"<ul> <li>TCP Traffic Shifting by Istio</li> </ul>"},{"location":"operation/coredump/","title":"6. Manage core dumps in GKE","text":""},{"location":"operation/coredump/#description","title":"Description","text":"<p>The issue with managing the core dumps in each pod, in case the Pod/Container/Node restart\u2026 then we will be losing all the data which was generated in core-dump folder, those are important data which cannot be compromised.  By default the container is generating the core dumps in the path \u201c/var/lib/systemd/coredump\u201d, this path can vary as per the core dump pattern defined in your container. We can create the core dump location specified in the file \u201c/proc/sys/kernel/core_pattern\u201d by doing something like below.</p> <pre><code>mkdir -p /tmp/cores\nchmod a+rwx /tmp/cores\necho \u201c/tmp/cores/core.%e.%p.%h.%t\u201d &gt; /proc/sys/kernel/core_pattern\n</code></pre> <p>Here I will be describing how we can manage core dumps for a GKE Cluster and storage core dumps into GCS for further process.</p>"},{"location":"operation/coredump/#guide","title":"Guide","text":"<pre><code>\n# 1. Create service account to access storage bucket.\n# 2. Create a Google Storage bucket.\n# 3. Persistent volume mount on core dumps location inside application container.\n# 4. Build a core dumps agent to collect dump files.\n# 5. Deploy core dump agent as a daemonset.\n# 6. Run a sample application to generate core dump. \n\n</code></pre>"},{"location":"operation/coredump/#references","title":"References","text":"<ul> <li>Core Dump handler</li> <li>How to generate coredump for containers running with k8s</li> </ul>"},{"location":"operation/fsnotify-pv/","title":"4. Monitoring filesystem through Inotify","text":""},{"location":"operation/fsnotify-pv/#description","title":"Description","text":"<p>Sometimes monitoring the status of a file is important for an application, such as triggering a hot reloading, related actions, etc. You can get a notification through 'Inotify' in Linux when operating a file, it also works for Pod mounted with Cloud Filestore or persistem storage. Following demo tried to show you how it works. </p> <p>However it's not working for object filesystem, which is not operating through the traditional filesystem API.</p>"},{"location":"operation/fsnotify-pv/#guide","title":"Guide","text":"<pre><code>git clone https://github.com/cc4i/awesome-gke.git\ncd awesome-gke/asset/inotify\n\n# 1. Create a storage class &amp; PVC, which takes a while to provision (minutes)\nkubectl apply -f manifests/pv.yaml\n\n# 2. Run Pod\nkubectl apply -f manifests/golang-deploy.yaml\n\n# 3. Copy the testing program\nkubectl get pods\n# Example output:\n# NAME                            READY   STATUS    RESTARTS   AGE\n# golang-1-6f65477fc4-n4q98       1/1     Running   0          23h\n# golang-2-5598d8996f-jhnvx       1/1     Running   0          23h\n\n# Replace the pod name\nkubectl cp ../inotify golang-1-6f65477fc4-n4q98:/go/inotify\nkubectl exec -it pods/golang-1-6f65477fc4-n4q98 -- bash\ncd /go/inotify\ngo mod tidy\ngo run main.go /tmp1\n\n# 4. Open other termial, and validating Inotify. You should be able to see operations after type 'touch cc'\nkubectl exec -it pods/golang-2-5598d8996f-jhnvx -- bash\ncd /tmp1\ntouch cc\n\n</code></pre>"},{"location":"operation/fsnotify-pv/#limitation","title":"Limitation","text":"<p>Inotify reports only events that a user-space program triggers through the filesystem API.  As a result, it does not catch remote events that occur on network filesystems.  (Applications must fall back to polling the filesystem to catch such events.) Furthermore, various pseudo-filesystems such as /proc, /sys, and /dev/pts are not monitorable with inotify.</p>"},{"location":"operation/fsnotify-pv/#references","title":"References","text":"<ul> <li>https://github.com/fsnotify/fsnotify</li> <li>https://github.com/ofek/csi-gcs</li> <li>https://man7.org/linux/man-pages/man7/inotify.7.html#:~:text=When%20a%20directory%20is%20monitored,referring%20to%20the%20inotify%20instance.</li> </ul>"},{"location":"operation/gke-login/","title":"2. Access Kubernetes API server without gcloud","text":""},{"location":"operation/gke-login/#description","title":"Description","text":"<p>Sometimes you want to login into GKE in the environment where doesn't have installed gcloud, the following guide will help you achieve that for sure.</p>"},{"location":"operation/gke-login/#method-1-use-the-key-of-a-serveice-account","title":"Method 1: Use the key of a Serveice Account","text":""},{"location":"operation/gke-login/#method-2-gnerate-kubeconfig-from-a-existed-kubeconfig","title":"Method 2: Gnerate kubeConfig from a existed kubeConfig","text":""},{"location":"operation/gke-login/#reference","title":"Reference","text":"<p>https://ahmet.im/blog/authenticating-to-gke-without-gcloud/ https://axe.me/blog/20190928173846_connect-to-gke-without-gcloud/</p>"},{"location":"operation/gpm/","title":"1. Improve Observeabiltiy with Google Cloud Managed Prometheus","text":""},{"location":"operation/gpm/#description","title":"Description","text":"<p>Build observability with Google Cloud Managed Service for Prometheus, which is built on top of Monarch, the same globally scalable data store used for Google's own monitoring.</p>"},{"location":"operation/gpm/#guide","title":"Guide","text":""},{"location":"operation/gpm/#references","title":"References","text":"<ul> <li>Google Cloud Managed Service for Prometheus</li> <li>Ingestion and querying with managed and self-deployed collection</li> </ul>"},{"location":"operation/remove-node/","title":"3. The simplest way to delete a node","text":""},{"location":"operation/remove-node/#decription","title":"Decription","text":"<p>Somehow you want to delete a specific node from the node pool in GKE cluster, but you can't do that in the console or through 'gcloud container'. Normally you probably cordon and drain the node, and then delete the instance as a normal VM, howerver you can do that through a single command from managed instance group. The node pool in GKE is related to a specific managed instance group, that why we can leverage command like 'gcloud compute instance-groups managed delete-instances'.</p>"},{"location":"operation/remove-node/#guide","title":"Guide","text":"<pre><code>\n# Provison a simple zonal cluster with 3 nodes\ngcloude containers create &lt;Cluster Name&gt; \\\n    --zone &lt;Zone&gt;\n\n# List MIG &amp; nodes\ngcloud container node-pools list --cluster &lt;Cluster Name&gt; \\\n    --zone &lt;Zone&gt;\n\n# Delete a specific node through 'gcloud compute instance-groups managed delete-instances'\ngcloud compute instance-groups managed delete-instances &lt;Managed Instance Group&gt; \\\n    --instances=&lt;Name of Instance1, Name of Instance2, ...&gt; \\\n    --zone &lt;Zone&gt;\n\n</code></pre>"},{"location":"operation/remove-node/#clean-up","title":"Clean up","text":"<pre><code>\n# Delete the GKE cluster\ngcloud container clusters delete &lt;Name of Cluster&gt; \\\n    --zone &lt;Zone&gt;\n    --async\n\n</code></pre>"},{"location":"operation/remove-node/#references","title":"References","text":"<ul> <li>https://pminkov.github.io/blog/removing-a-node-from-a-kubernetes-cluster-on-gke-google-container-engine.html</li> </ul>"},{"location":"operation/transfer-pv/","title":"5. Trasfer PV across zone","text":""},{"location":"operation/transfer-pv/#description","title":"Description","text":"<p>If you use StorageClass to provision regional PV in GKE, your Pod can mount PV back even it's reboot in node at different zona. However when you use zonal persistent disk to provision your PV (by default), and then you need to manually tranfer your storage across zone. Here's guide to help you transfer your zonal PV in GKE. </p>"},{"location":"operation/transfer-pv/#guide","title":"Guide","text":""},{"location":"operation/transfer-pv/#1-take-a-snapshot-for-your-orginal-volumn","title":"1. Take a snapshot for your orginal volumn.","text":""},{"location":"operation/transfer-pv/#2-create-a-new-disk-using-the-snapthot","title":"2. Create a new disk using the snapthot.","text":""},{"location":"operation/transfer-pv/#3-create-a-new-pv-and-pvc-from-the-disk","title":"3. Create a new PV and PVC from the disk.","text":""},{"location":"operation/transfer-pv/#4-launch-a-example-deployment-using-pvc-created-above","title":"4. Launch a example deployment using PVC created above.","text":""},{"location":"operation/transfer-pv/#clean-up","title":"Clean up","text":""},{"location":"operation/transfer-pv/#references","title":"References","text":"<ul> <li>Create clones of persistent volumes</li> <li>Create a snapshot of a zonal persistent disk</li> <li>Persistent volumes and dynamic provisioning</li> </ul>"},{"location":"scaling/block-scaling/","title":"1. Solve the issue of blocking scale-down","text":""},{"location":"scaling/block-scaling/#description","title":"Description","text":"<p>Sometimes you may face the issue like \"Pod is blocking scale down because it has local storage\" in GKE, or doesn't have enough enough Pod Disruption Budget, those issues could be solved in following ways:</p> <ul> <li> <p>Adding the parameter \"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"true\" to your Pods</p> </li> <li> <p>Specifying a Pod Disruption Budget for your Pods</p> </li> <li> <p>Check out Node-level reason messages for noScaleDown events appear in the noDecisionStatus.noScaleDown.nodes[].reason field and take action accordingly.</p> </li> </ul>"},{"location":"scaling/block-scaling/#refernces","title":"Refernces","text":"<ul> <li> <p>NoScaleDown node-level reasons</p> </li> <li> <p>How does scale-down work?</p> </li> <li> <p>Does Cluster autoscaler work with PodDisruptionBudget in scale-down?</p> </li> <li> <p>What types of Pods can prevent Cluster autoscaler from removing a node?</p> </li> </ul>"},{"location":"security/slsa/","title":"2. Secure CI/CD following SLSA","text":""},{"location":"security/slsa/#description","title":"Description","text":"<p>Build a secure CI/CD to avoid supply chain attacks occur along the path of the software development lifecycle, where are many injection points. </p> <p>In Google Cloud there are various services and products, which provide you capability to build your own secure CI/CD, the following example is tend to demostrate how it works.</p>"},{"location":"security/slsa/#supply-chain-blueprint-on-google-cloud","title":"Supply Chain Blueprint on Google Cloud","text":""},{"location":"security/slsa/#cicd-demonstration","title":"CI/CD Demonstration","text":"<p>TODO</p>"},{"location":"security/slsa/#references","title":"References","text":"<ul> <li> <p>What's new in cloud-native CI/CD </p> </li> <li> <p>Software Delivery Shield</p> </li> </ul>"},{"location":"security/workload-idetity/","title":"1. Acccessing AWS services from GKE through workload identity","text":""},{"location":"security/workload-idetity/#description","title":"Description","text":"<p>Securely acccess AWS services from GKE through Workload Identity without using long-living AWS credentials (AKSK), we leverage gtoken to automatically inject AWS credentials into pods.</p>"},{"location":"security/workload-idetity/#prerequisite","title":"Prerequisite","text":"<ul> <li>AWS CLI</li> <li>GCLOUD CLI</li> <li>Enable Workload Identity on GKE</li> <li>Configure application to use Workload Identity</li> </ul>"},{"location":"security/workload-idetity/#deployment","title":"Deployment","text":"<pre><code># 0. Configure environment variables for following steps\nexport PROJECT_ID=&lt;Project ID&gt;\nexport CLUSTER_NAME=&lt;The name of GKE cluster&gt;\nexport GSA_NAME=&lt;The name of Service Account&gt;\nexport GSA_ID=$(gcloud iam service-accounts describe --format json ${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com  | jq -r '.uniqueId')\nexport KSA_NAME=&lt;The name of Service Account in Kubernetes&gt;\nexport KSA_NAMESPACE=&lt;The name of Namespace where the Service Account is located&gt;\nexport AWS_ROLE_NAME=&lt;The name of AWS role&gt;\nexport AWS_POLICY_NAME=&lt;The name of policy that was granted to AWS role&gt;\n\n\n# 1. Create the Service Account and grant necessary permissions\ngcloud iam service-accounts create ${GSA_NAME} \\\n    --description=\"Service account for Workload Identity and test accessing AWS resource.\" \\\n    --display-name=${GSA_NAME}\n\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n    --member=\"serviceAccount:${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n    --role=\"roles/container.admin\" \\\n    --role=\"roles/resourcemanager.projectIamAdmin\"\n\n\n# 2. Create the AWS role for demo purpose (Customize your role if need to)\n# Add trusted principal for Google account\ncat &gt; gcp-trust-policy.json &lt;&lt; EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"accounts.google.com\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"accounts.google.com:sub\": \"${GSA_ID}\"\n        }\n      }\n    }\n  ]\n}\nEOF\n# Create the AWS role \naws iam create-role --role-name ${AWS_ROLE_NAME} --assume-role-policy-document file://gcp-trust-policy.json\n# Attache the policy to the AWS role\naws iam attach-role-policy --role-name ${AWS_ROLE_NAME} --policy-arn arn:aws:iam::aws:policy/${AWS_POLICY_NAME}\n# Retrieve ARN from the AWS role\nexport AWS_ROLE_ARN=$(aws iam get-role --role-name ${AWS_ROLE_NAME} --query Role.Arn --output text)\n\n\n# 3. Create the namespace and related services account in GKE\n# Create the namespace\nkubectl create namespace ${KSA_NAMESPACE}\n# Create the service account\nkubectl create serviceaccount --namespace ${KSA_NAMESPACE} ${KSA_NAME}\n\n# Binding Service Account with Workload Identity\ngcloud iam service-accounts add-iam-policy-binding ${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com \\\n    --role roles/iam.workloadIdentityUser \\\n    --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[${KSA_NAMESPACE}/${KSA_NAME}]\"\n\nkubectl annotate serviceaccount --namespace ${KSA_NAMESPACE} ${KSA_NAME} \\\n  iam.gke.io/gcp-service-account=${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\n\nkubectl annotate serviceaccount --namespace ${KSA_NAMESPACE} ${KSA_NAME} \\\n  amazonaws.com/role-arn=${AWS_ROLE_ARN}\n\n\n# 4. Run demo to verify services accessing\ncat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\n  namespace: ${KSA_NAMESPACE}\nspec:\n  serviceAccountName: ${KSA_NAME}\n  containers:\n  - name: test-pod\n    image: amazon/aws-cli\n    command: [\"tail\", \"-f\", \"/dev/null\"]\nEOF\n\n# Login into Pod Shell\nkubectl exec -it pods/test-pod -- sh\n# In Pod shell: check AWS assumed role\naws sts get-caller-identity\n\n\n</code></pre>"},{"location":"security/workload-idetity/#references","title":"References","text":"<ul> <li>https://github.com/doitintl/gtoken</li> </ul>"},{"location":"solutions/acm-kpt/","title":"2. Manage GKEs with Anthos Config Management & kpt","text":""},{"location":"solutions/acm-kpt/#description","title":"Description","text":"<p>The tutorial is to compose the Kubernetes Resource Model (KRM) blueprints with Anthos Config Management and provision a Google Kubernetes Engine (GKE) cluster and the required networking infrastructure such as a Virtual Private Cloud (VPC) and a subnet to host the GKE cluster, and named IP ranges for pods and services. </p>"},{"location":"solutions/acm-kpt/#prerequisite","title":"Prerequisite","text":"<ul> <li>gcloud</li> <li>kpt</li> <li>kubectl</li> </ul>"},{"location":"solutions/acm-kpt/#steps","title":"Steps","text":"<pre><code>\n# 1. Setup environment variables\nexport PROJECT_ID=play-with-anthos-340801\nexport ACM_CONTROLLER=acm-controller\nexport REGION=us-central1\nexport CONFIG_NAMESPACE=config-control\nexport VPC_NAME=gke-test-vpc\nexport SUBNET_NAME=${VPC_NAME}-subnetwork-${REGION}\nexport GKE_CLUSTER=acm-kpt-cluster\n\n# 2. Enable APIs\ngcloud services enable serviceusage.googleapis.com \\\n    krmapihosting.googleapis.com \\\n    container.googleapis.com \\\n    cloudresourcemanager.googleapis.com\n\n# 3. Create Anthos Config Controller, GKE cluster could be private with NAT, more detail - https://cloud.google.com/sdk/gcloud/reference/anthos/config/controller/create\ngcloud anthos config controller create ${ACM_CONTROLLER} \\\n    --location=${REGION}\n\n# 4. Give Config Controller permission to manage Google Cloud resources\nexport SA_EMAIL=\"$(kubectl get ConfigConnectorContext -n config-control \\\n    -o jsonpath='{.items[0].spec.googleServiceAccount}' 2&gt; /dev/null)\"\ngcloud projects add-iam-policy-binding \"${PROJECT_ID}\" \\\n    --member \"serviceAccount:${SA_EMAIL}\" \\\n    --role \"roles/owner\" \\\n    --project \"${PROJECT_ID}\"\n\n\n# 5. Verify that Config Connector is configured and healthy in the project namespace\nkubectl get ConfigConnectorContext -n ${CONFIG_NAMESPACE} \\\n    -o \"custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,HEALTHY:.status.healthy\"\n\n# Configure the VPC\nkpt pkg get \\\n  https://github.com/cc4i/awesome-gke.git/asset/acm-kpt/vpc@main \\\n  ${VPC_NAME}\ncd ${VPC_NAME}\n( echo \"cat &lt;&lt;EOF\" ; cat setters.yaml ; echo EOF ) | sh &gt; setters-val.yaml\nmv setters-val.yaml setters.yaml\n# kpt fn eval --image set-namespace:v0.1 -- namespace=config-control\n\n\n# 6. Configure subnet \nkpt pkg get \\\n  https://github.com/cc4i/awesome-gke.git/asset/acm-kpt/subnet@main \\\n  ${SUBNET_NAME}\ncd ${SUBNET_NAME}\n( echo \"cat &lt;&lt;EOF\" ; cat setters.yaml ; echo EOF ) | sh &gt; setters-val.yaml\nmv setters-val.yaml setters.yaml\n# kpt fn eval --image set-namespace:v0.1 -- namespace=config-control\n\n\n# 7. Initialize the working directory with kpt, which creates a resource to track changes\ncd ..\nkpt fn render\nkpt live init --namespace config-control\nkpt live apply --dry-run\nkpt live apply\nkpt live status --output table --poll-until current\n\n# 8. Configure GKE\ncd ..\nkpt pkg get \\\n  https://github.com/cc4i/awesome-gke.git/asset/acm-kpt/gke@main \\\n  ${GKE_CLUSTER}\ncd ${GKE_CLUSTER}\n( echo \"cat &lt;&lt;EOF\" ; cat setters.yaml ; echo EOF ) | sh &gt; setters-val.yaml\nmv setters-val.yaml setters.yaml\nkpt fn render\nkpt live init --namespace config-control\nkpt live apply --dry-run\nkpt live apply\nkpt live status --output table --poll-until current\n\n</code></pre>"},{"location":"solutions/acm-kpt/#clean-up","title":"Clean Up","text":"<pre><code>cd ${GKE_CLUSTER}\nkpt live destroy\n\ncd ${VPC_NAME}\nkpt live destroy\n\n</code></pre>"},{"location":"solutions/acm-kpt/#references","title":"References","text":"<ul> <li>KRM Blueprints</li> <li>GKE Cluster Bluepint</li> <li>Blueprint Catalog in Github</li> </ul>"},{"location":"solutions/es-gke/","title":"3. High scalable and available Elasticsearch on GKE","text":""},{"location":"solutions/es-gke/#description","title":"Description","text":"<p>Highly scalable and available Elasticsearch on GKE.</p>"},{"location":"solutions/es-gke/#architecture","title":"Architecture","text":""},{"location":"solutions/es-gke/#guide","title":"Guide","text":""},{"location":"solutions/es-gke/#stress-testing","title":"Stress Testing","text":""},{"location":"solutions/es-gke/#clean-up","title":"Clean Up","text":""},{"location":"solutions/es-gke/#references","title":"References","text":"<ul> <li>https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html</li> <li>https://github.com/logzio/elasticsearch-stress-test</li> </ul>"},{"location":"solutions/kafka-gke/","title":"Kafka gke","text":""},{"location":"solutions/kafka-gke/#description","title":"Description","text":""},{"location":"solutions/kafka-gke/#references","title":"References","text":"<ul> <li>Kafka Component Architecture</li> <li>Kafka brokers and topics</li> <li>Strimzi Kafka Operator</li> </ul>"},{"location":"solutions/play-k3s/","title":"1. K3s integrate Google Cloud Provider on GCE","text":""},{"location":"solutions/play-k3s/#description","title":"Description","text":"<p>K3s is a fully compliant Kubernetes distribution and packaged as a single binary. Easy to install, half the memory, all in a binary of less than 100 MB, which is great for: - Edge - IoT - CI - Development - ARM - Embedding K8s - Situations where a PhD in K8s clusterology is infeasible</p> <p>More detail could be found here. K3s can be easily deployed into GCP but you need to put some extra effort in order to achieve a production-ready cluster, try to address that as following.</p>"},{"location":"solutions/play-k3s/#all-in-one-quick-start-example","title":"All-in-one quick-start example","text":"<p>The k3s cluster uses managed instance group for both server and agent nodes for high available purpose, but no auto scaling capability, no external database.</p> <pre><code>git clone https://github.com/cc4i/awesome-gke.git\ncd awesome-gke/asset/k3s/bin &amp;&amp; ./k3s-quickstart.sh\n</code></pre>"},{"location":"solutions/play-k3s/#high-availability-and-auto-scalability-k3s-cluster","title":"High-Availability and auto-scalability K3s Cluster","text":"<p>To build a highly avaiable and scalable K3s cluster on GCP:</p> <ul> <li> <p>Leveraged the managed instance group for both server and agent (control plane and worker node pool).</p> </li> <li> <p>Integrated MIGs with Cloud Load Balancing with reserved IP to provide fixed registeration address. </p> </li> <li> <p>Configure auto scaling policy with CPU utilization by default for MIGs, that provides underneath capability of scaling. </p> </li> <li> <p>Using Cloud SQL as external database for K3s.</p> </li> </ul> <p>Architectural diagram </p> <p>Deployment</p> <pre><code>cd awesome-gke/asset/k3s/bin &amp;&amp; ./k3s-ha.sh\n</code></pre>"},{"location":"solutions/play-k3s/#integrate-with-cloud-controller-manager-ccm-for-gce","title":"Integrate with Cloud Controller Manager (CCM) for GCE","text":"<p>Keys</p> <ul> <li> <p>Grant server &amp; agent nodes with permission in order to provision network resources.</p> </li> <li> <p>Allow port:60000 in the firewall rules.</p> </li> <li> <p>Following guidance to build Cloud Controller Manager (CCM) and push into your own image registry.</p> </li> <li> <p>Configure RBAC and allow CCM working properly, see reference.</p> </li> </ul> <p>Steps</p> <ul> <li> <p>Create service account with proper permissions.</p> </li> <li> <p>Create instance template for K3s server.</p> </li> <li> <p>Create managed instance group with server template.</p> </li> <li> <p>Install K3s server side.</p> </li> <li> <p>Create agent instances template for K3s agent.</p> </li> <li> <p>Cerate managed instance group with agent template.</p> </li> <li> <p>Taint server node.</p> </li> <li> <p>Deploy CCM for GCE into K3s cluster.</p> </li> </ul>"},{"location":"solutions/play-k3s/#clean-up","title":"Clean up","text":"<pre><code>cd awesome-gke/hack &amp;&amp; ./k3s-quickstart-cleanup.sh\n</code></pre>"},{"location":"workaround/autopilot-storage/","title":"3. Three ways to surpass ephemeral storage limit of GKE Autopilot","text":""},{"location":"workaround/autopilot-storage/#description","title":"Description","text":"<p>In GKE Autopilot the ephemeral storage limit must be within 10 MiB and 10 GiB for all compute classes, it's a potential issue for those applicaitons need big size storage. </p> <p>There are three diffrent way to surpass the limit and you can choose one of those based on your scenario.</p>"},{"location":"workaround/autopilot-storage/#use-memory-for-emptydir","title":"Use memory for emptyDir","text":"<p>Ephmeral storage could be emptyDir, be default there's limit 10G, howerver emptyDir has different type, which could be host file system or memory directly, so we can use this kind of type to surpass 10G limit (memory limit is up to 80G at the moment). However it's kind of expensive and also CPU and Memory must be match ratio between 1:1 to 1:6.5.</p> <p>For example, create 15Gi ephemeral storage that's going to occupied half of total memory 30Gi.</p> <pre><code>\n# Create Pod in Autopilot with emptyDir\nkubectl apply -f - &lt;&lt; __EOF__\napiVersion: v1\nkind: Pod\nmetadata:\n  name: buildah-emptydir \nspec:\n  containers:\n    - name: buildah\n      image: quay.io/buildah/stable:v1.23.1\n      command: [\"sleep\", \"infinity\"]  \n      volumeMounts:\n      - mountPath: /var/lib/containers\n        name: container-storage\n      resources:\n        requests:\n          cpu: 5000m\n          memory: 30Gi\n  volumes:\n  - name: container-storage\n    emptyDir:\n     medium: Memory\n     sizeLimit: 15Gi\n__EOF__\n\n# Validate the size of ephemeral storage\nkubectl exec -it pods/buildah-emptydir -- bash\ndf -h\n\n</code></pre>"},{"location":"workaround/autopilot-storage/#use-cloud-filestore","title":"Use Cloud Filestore","text":"<p>By default StorageClass for Filestore and Persistent Disk were been installed when you launched the GKE Autopilot. For example, create 1Ti storage with Filestore through StorageClass dynamically.</p> <pre><code># Apply storage class\nkubectl apply -f - &lt;&lt; __EOF__\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: filestore-test\nprovisioner: filestore.csi.storage.gke.io\nvolumeBindingMode: Immediate\nallowVolumeExpansion: true\nparameters:\n  tier: standard\n  network: default\n__EOF__\n\n\n# Provision PVC through StorageClass dynamically \nkubectl apply -f - &lt;&lt; __EOF__\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: podpvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: filestore-test\n  resources:\n    requests:\n      storage: 1Ti\n__EOF__\n\n# Using the storage in following Nginx example\nkubectl apply -f - &lt;&lt; __EOF__\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html\n          name: mypvc\n      volumes:\n      - name: mypvc\n        persistentVolumeClaim:\n          claimName: podpvc\n__EOF__\n\n</code></pre>"},{"location":"workaround/autopilot-storage/#use-persistent-disk","title":"Use Persistent Disk","text":"<p>This example is going to provision 30Gi storage through PD storage class and mount by Nginx Pod.</p> <pre><code># Check out default storage classes\nkubectl get storageClass -o wide\n\n# Provison 30G PD through storageClass\nkubectl apply -f - &lt;&lt; __EOF__\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc-demo\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 30Gi\n  storageClassName: standard-rwo\n__EOF__\n\n# Using the storage in following Nginx example\nkubectl apply -f - &lt;&lt; __EOF__\nkind: Pod\napiVersion: v1\nmetadata:\n  name: pod-demo\nspec:\n  volumes:\n    - name: pvc-demo-vol\n      persistentVolumeClaim:\n       claimName: pvc-demo\n  containers:\n    - name: pod-demo\n      image: nginx\n      resources:\n        limits:\n          cpu: 10m\n          memory: 80Mi\n        requests:\n          cpu: 10m\n          memory: 80Mi\n      ports:\n        - containerPort: 80\n          name: \"http-server\"\n      volumeMounts:\n        - mountPath: \"/usr/share/nginx/html\"\n          name: pvc-demo-vol\n__EOF__\n</code></pre>"},{"location":"workaround/autopilot-storage/#allowed-storage-type","title":"Allowed Storage type","text":"<ul> <li>\"configMap\"</li> <li>\"csi\"</li> <li>\"downwardAPI\"</li> <li>\"emptyDir\"</li> <li>\"gcePersistentDisk\"</li> <li>\"hostPath\",</li> <li>\"nfs\"</li> <li>\"persistentVolumeClaim\"</li> <li>\"projected\"</li> <li>\"secret\"</li> </ul>"},{"location":"workaround/autopilot-storage/#refernces","title":"Refernces","text":"<ul> <li>https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview</li> </ul>"},{"location":"workaround/custom-autoscaler/","title":"2. Use customized Cluster Autoscaler instead of built-in one from GKE","text":""},{"location":"workaround/custom-autoscaler/#description","title":"Description","text":"<p>The GKE has their own comprehensive capability of scaling, which is capabale enough to build highly scalable cluster. However some customers are building things with upstream Cluster Autoscaler, especailly for those of you migrated from other cloud providers who don't want to change their familiar experiences depends on upstream Cluster Autoscaler, or some features don't match your expectation such as expander, configuring terminationGracePeriodSeconds, etc., following content is trying to address that.</p>"},{"location":"workaround/custom-autoscaler/#using-an-expander","title":"Using an expander","text":""},{"location":"workaround/custom-autoscaler/#configuring-terminationgraceperiodseconds","title":"Configuring terminationGracePeriodSeconds","text":"<p>To deploy a customized Cluster Autoscaler instead of use built-in one from GKE and configure termination grace period.</p>"},{"location":"workaround/startup-script/","title":"1. Run a boortrap script when launching nodes in GKE","text":""},{"location":"workaround/startup-script/#description","title":"Description","text":"<p>In GKE Standard you can't run customized startup script when booting up a node due to not support customized host operation system (OS), fortunately we can use DaemonSet to address this issue. The following example tried to demostrate how to run a startup script through DeamonSet, you can build you own as the reference here.</p>"},{"location":"workaround/startup-script/#deployment","title":"Deployment","text":"<pre><code>\n# Clone repo\ngit clone https://github.com/cc4i/awesome-gke.git\ncd asset/startup-script\n\n# build image &amp; push into registry\ndocker build . -t &lt;image url&gt;:&lt;tag&gt;\ndocker push &lt;image url&gt;:&lt;tag&gt;\n\n# Modify script as per request betweend \"# BO:\" and \"# EO:\"\n\n# Deploy to cluster\nkustomize build ./manifests | kubectl apply -f -\n\n</code></pre>"},{"location":"workaround/startup-script/#notes","title":"Notes","text":"<ul> <li>The pod container can share the host process ID namespace.</li> <li>THe container is required root privilege.</li> </ul>"}]}